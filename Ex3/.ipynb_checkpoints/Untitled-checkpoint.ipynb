{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "speaking-punch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils as u\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "designed-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequential: Creates a linear stack of layers\n",
    "#Drouput: Ensures minimum overfitting. it does this my selecting random nodes and setting them to 0\n",
    "#Dense: This essentially is the output layer. It performs the output = activation(dot(input, weights) + bias)\n",
    "#Flatten: This rolls out our array into 2 dimensions, [numberOfData, features]\n",
    "#SGD: Stochastic Gradient Descent, this is the optimizer\n",
    "#Conv2D: This is the convolution layer\n",
    "#MaxPooling2D: This function performs max pooling\n",
    "#np_utils: Some tools to allow us to format our data\n",
    "#cifar10: This is the dataset we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "insured-vertex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [8]\n",
      " [8]\n",
      " ...\n",
      " [5]\n",
      " [1]\n",
      " [7]]\n"
     ]
    }
   ],
   "source": [
    " #Lets start by loading the Cifar10 data\n",
    "(X, y), (X_test, y_test) = cifar10.load_data()\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "respected-theta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Keep in mind the images are in RGB\n",
    "#So we can normalise the data by diving by 255\n",
    "#The data is in integers therefore we need to convert them to float first\n",
    "X, X_test = X.astype('float32')/255.0, X_test.astype('float32')/255.0\n",
    "#Then we convert the y values into one-hot vectors\n",
    "#The cifar10 has only 10 classes, thats is why we specify a one-hot\n",
    "#vector of width/class 10\n",
    "\n",
    "y, y_test = u.to_categorical(y, 10), u.to_categorical(y_test, 10)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "found-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Now we can go ahead and create our Convolution model\n",
    "model = Sequential()\n",
    "#We want to output 32 features maps. The kernel size is going to be\n",
    "#3x3 and we specify our input shape to be 32x32 with 3 channels\n",
    "#Padding=same means we want the same dimensional output as input\n",
    "#activation specifies the activation function\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), padding='same',\n",
    "          activation='relu'))\n",
    "#20% of the nodes are set to 0\n",
    "model.add(Dropout(0.2))\n",
    "#now we add another convolution layer, again with a 3x3 kernel\n",
    "#This time our padding=valid this means that the output dimension can\n",
    "#take any form\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='valid'))\n",
    "#maxpool with a kernet of 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#In a convolution NN, we neet to flatten our data before we can\n",
    "#input it into the ouput/dense layer\n",
    "model.add(Flatten())\n",
    "#Dense layer with 512 hidden units\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#this time we set 30% of the nodes to 0 to minimize overfitting\n",
    "model.add(Dropout(0.3))\n",
    "#Finally the output dense layer with 10 hidden units corresponding to\n",
    "#our 10 classe\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "#Few simple configurations\n",
    "model.compile(loss='categorical_crossentropy',optimizer=SGD(momentum=0.5, decay=0.0004))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fuzzy-idaho",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "98/98 [==============================] - 56s 570ms/step - loss: 1.2252 - val_loss: 1.2461\n",
      "Epoch 2/25\n",
      "98/98 [==============================] - 59s 603ms/step - loss: 1.2193 - val_loss: 1.2464\n",
      "Epoch 3/25\n",
      "98/98 [==============================] - 54s 551ms/step - loss: 1.2143 - val_loss: 1.2394\n",
      "Epoch 4/25\n",
      "98/98 [==============================] - 53s 543ms/step - loss: 1.2079 - val_loss: 1.2372\n",
      "Epoch 5/25\n",
      "98/98 [==============================] - 58s 595ms/step - loss: 1.2044 - val_loss: 1.2324\n",
      "Epoch 6/25\n",
      "98/98 [==============================] - 57s 585ms/step - loss: 1.2030 - val_loss: 1.2342\n",
      "Epoch 7/25\n",
      "98/98 [==============================] - 55s 558ms/step - loss: 1.2006 - val_loss: 1.2275\n",
      "Epoch 8/25\n",
      "98/98 [==============================] - 57s 579ms/step - loss: 1.1960 - val_loss: 1.2299\n",
      "Epoch 9/25\n",
      "98/98 [==============================] - 59s 601ms/step - loss: 1.1911 - val_loss: 1.2259\n",
      "Epoch 10/25\n",
      "98/98 [==============================] - 58s 589ms/step - loss: 1.1897 - val_loss: 1.2254\n",
      "Epoch 11/25\n",
      "98/98 [==============================] - 59s 601ms/step - loss: 1.1848 - val_loss: 1.2258\n",
      "Epoch 12/25\n",
      "98/98 [==============================] - 59s 601ms/step - loss: 1.1825 - val_loss: 1.2168\n",
      "Epoch 13/25\n",
      "98/98 [==============================] - 61s 619ms/step - loss: 1.1807 - val_loss: 1.2204\n",
      "Epoch 14/25\n",
      "98/98 [==============================] - 60s 610ms/step - loss: 1.1737 - val_loss: 1.2187\n",
      "Epoch 15/25\n",
      "98/98 [==============================] - 58s 596ms/step - loss: 1.1729 - val_loss: 1.2156\n",
      "Epoch 16/25\n",
      "98/98 [==============================] - 62s 631ms/step - loss: 1.1669 - val_loss: 1.2133\n",
      "Epoch 17/25\n",
      "98/98 [==============================] - 64s 656ms/step - loss: 1.1654 - val_loss: 1.2107\n",
      "Epoch 18/25\n",
      "98/98 [==============================] - 63s 639ms/step - loss: 1.1628 - val_loss: 1.2112\n",
      "Epoch 19/25\n",
      "98/98 [==============================] - 60s 616ms/step - loss: 1.1609 - val_loss: 1.2083\n",
      "Epoch 20/25\n",
      "98/98 [==============================] - 58s 596ms/step - loss: 1.1566 - val_loss: 1.2013\n",
      "Epoch 21/25\n",
      "98/98 [==============================] - 60s 612ms/step - loss: 1.1547 - val_loss: 1.2018\n",
      "Epoch 22/25\n",
      "98/98 [==============================] - 59s 605ms/step - loss: 1.1499 - val_loss: 1.2043\n",
      "Epoch 23/25\n",
      "98/98 [==============================] - 58s 587ms/step - loss: 1.1464 - val_loss: 1.2016\n",
      "Epoch 24/25\n",
      "98/98 [==============================] - 57s 586ms/step - loss: 1.1417 - val_loss: 1.2051\n",
      "Epoch 25/25\n",
      "98/98 [==============================] - 57s 580ms/step - loss: 1.1396 - val_loss: 1.1933\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 1.1933\n",
      "1 1.193320631980896\n"
     ]
    }
   ],
   "source": [
    "#Run the algorithm!\n",
    "model.fit(X, y, validation_data=(X_test, y_test), epochs=25,\n",
    "batch_size=512)\n",
    "#Save the weights to use for later\n",
    "model.save_weights(\"cifar10.hdf5\")\n",
    "#Finally print the accuracy of our model!\n",
    "#print(\"Accuracy: &2.f%%\" %(model.evaluate(X_test, y_test)[1]*100))\n",
    "print(\"1a\",model.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-comedy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
